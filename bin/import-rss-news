#!/usr/bin/env ruby
# frozen_string_literal: true

require 'yaml'
require 'rss'
require 'open-uri'
require 'net/http'
require 'openssl'
require 'fileutils'
require 'time'
require 'digest'
require 'reverse_markdown'
require 'nokogiri'
require 'thread'
require_relative '../lib/logging'

NEWS_DIR = '_posts'
SOURCES_DIR = '_organizations'
MAX_ITEM_AGE_DAYS = 365
MAX_FILENAME_BYTES = 255
CHECKSUM_FILE = ENV.fetch('RSS_CHECKSUM_FILE', File.expand_path('feed_checksums.yml', __dir__))
MAX_WORKERS = Integer(ENV.fetch('RSS_WORKERS', '6')) rescue 6
LOGGER = Logging.build_logger(env_var: 'LOG_LEVEL')

FileUtils.mkdir_p(NEWS_DIR)

# Helper to read frontmatter from a Markdown file
def read_frontmatter(file_path)
  content = File.read(file_path)
  if content =~ /\A---\s*\n(.*?\n?)^---\s*$\n?/m
    YAML.safe_load(Regexp.last_match(1), permitted_classes: [Date, Time]) || {}
  else
    {}
  end
rescue Psych::DisallowedClass => e
  LOGGER.warn "Skipping frontmatter in #{file_path}: #{e.message}"
  {}
end

ARTICLE_BODY_SELECTORS = [
  '#news_content_body',
  '[id*="news_content_body"]',
  '.news_content_body',
  '[class*="news_content_body"]',
  '.news-body',
  '.article-body',
  '.article__body',
  '.news-article__body',
  'article .body'
].freeze

DEFAULT_OPEN_TIMEOUT = Integer(ENV.fetch('RSS_OPEN_TIMEOUT', '5')) rescue 5
DEFAULT_READ_TIMEOUT = Integer(ENV.fetch('RSS_READ_TIMEOUT', '10')) rescue 10

def fetch_article_body_html(url)
  html = URI.open(url, open_timeout: DEFAULT_OPEN_TIMEOUT, read_timeout: DEFAULT_READ_TIMEOUT).read
  doc = Nokogiri::HTML(html)

  ARTICLE_BODY_SELECTORS.each do |selector|
    node = doc.at_css(selector)
    next unless node

    snippet = node.inner_html.to_s.strip
    return snippet unless snippet.empty?
  end

  fallback = doc.at_css('main') || doc.at_css('#main') || doc.at_css('#content')
  fallback&.inner_html&.strip
rescue OpenURI::HTTPError, OpenSSL::SSL::SSLError, SocketError,
       Net::OpenTimeout, Net::ReadTimeout, Timeout::Error => e
  LOGGER.warn "Failed to fetch article body (#{url}): #{e.message}"
  nil
rescue StandardError => e
  LOGGER.error "Unexpected error scraping #{url}: #{e.message}"
  nil
end

def published_at(item)
  candidates = []
  candidates << item.pubDate if item.respond_to?(:pubDate)
  candidates << item.dc_date if item.respond_to?(:dc_date)
  candidates << item.updated if item.respond_to?(:updated)
  candidates << item.date if item.respond_to?(:date)

  value = candidates.compact.first
  return value if value.is_a?(Time)

  if value.respond_to?(:to_time)
    value.to_time
  elsif value
    Time.parse(value.to_s)
  end
rescue StandardError
  nil
end

def stale_item?(published_time)
  cutoff = Time.now - (MAX_ITEM_AGE_DAYS * 24 * 60 * 60)
  published_time < cutoff
end

def sanitized_slug(text)
  text.to_s.downcase.gsub(/[^a-z0-9]+/, '-').gsub(/^-|-$/, '')
end

def filename_slug(title, link, date_prefix)
  base_slug = sanitized_slug(title)
  fallback_slug = Digest::SHA1.hexdigest(link.to_s)[0, 12]
  base_slug = fallback_slug if base_slug.empty?

  max_slug_length = MAX_FILENAME_BYTES - "#{date_prefix}-".length - '.md'.length
  max_slug_length = 1 if max_slug_length < 1

  return base_slug if base_slug.length <= max_slug_length

  digest = Digest::SHA1.hexdigest(link.to_s)[0, 8]
  truncated_length = max_slug_length - digest.length - 1
  truncated_length = 0 if truncated_length.negative?
  truncated = base_slug[0, truncated_length]
  slug = [truncated, digest].reject(&:empty?).join('-')
  slug = fallback_slug if slug.empty?
  slug.length > max_slug_length ? slug[0, max_slug_length] : slug
end

def load_feed_checksums
  return {} unless File.exist?(CHECKSUM_FILE)

  data = YAML.safe_load(File.read(CHECKSUM_FILE))
  data.is_a?(Hash) ? data : {}
rescue StandardError => e
  LOGGER.warn "Failed to load feed checksum cache (#{CHECKSUM_FILE}): #{e.message}"
  {}
end

def save_feed_checksums(data)
  File.write(CHECKSUM_FILE, data.to_yaml)
end

def item_content_html(item)
  return item.content_encoded if item.respond_to?(:content_encoded) && item.content_encoded
  return item.description if item.respond_to?(:description) && item.description
  return item.summary if item.respond_to?(:summary) && item.summary

  content = item.content if item.respond_to?(:content)
  return content.content if content.respond_to?(:content) && content.content
  content if content.is_a?(String)
rescue StandardError => e
  LOGGER.warn "Failed to read content for #{item.link || item.title}: #{e.message}"
  nil
end

def item_title_text(item)
  title = item.title if item.respond_to?(:title)
  return title.content if title.respond_to?(:content)
  title.to_s
rescue StandardError
  item.link || 'Untitled'
end

def item_link_url(item)
  if item.respond_to?(:link)
    link = item.link
    return link.href if link.respond_to?(:href)
    return link.to_s unless link.is_a?(RSS::Atom::Feed::Link) # avoid dumping object graphs
  end

  if item.respond_to?(:links) && item.links.respond_to?(:each)
    alternate = item.links.find { |l| l.respond_to?(:rel) && l.rel == 'alternate' }
    return alternate.href if alternate && alternate.respond_to?(:href)

    first = item.links.first
    return first.href if first && first.respond_to?(:href)
  end

  item.respond_to?(:url) ? item.url.to_s : nil
rescue StandardError => e
  LOGGER.warn "Failed to read link for #{item.respond_to?(:title) ? item.title : 'unknown item'}: #{e.message}"
  nil
end

def feed_summary_line(source_title, rss_url, stats)
  labels = {
    created: 'created',
    duplicates: 'duplicates',
    stale: 'stale',
    missing_link: 'missing_link',
    missing_title: 'missing_title',
    missing_publish_date: 'missing_date',
    empty_content: 'no_content'
  }

  parts = labels.map do |key, label|
    value = stats[key]
    "#{label}=#{value}" if value&.positive?
  end.compact

  status = parts.empty? ? 'no_changes' : parts.join(', ')
  "Processed '#{source_title}' (#{rss_url}): #{status}"
end

feed_checksums = load_feed_checksums
queue = Queue.new
Dir.glob("#{SOURCES_DIR}/*.md").each { |source_file| queue << source_file }
lock = Mutex.new

workers = Array.new([MAX_WORKERS, 1].max) do
  Thread.new do
    loop do
      source_file = begin
        queue.pop(true)
      rescue ThreadError
        break
      end

      frontmatter = read_frontmatter(source_file)
      rss_url = frontmatter['news_rss_url']
      source_title = frontmatter['title']
      next unless rss_url

      begin
        stats = Hash.new(0)
        rss_content = URI.open(rss_url, open_timeout: DEFAULT_OPEN_TIMEOUT, read_timeout: DEFAULT_READ_TIMEOUT, &:read)
        checksum = Digest::SHA256.hexdigest(rss_content)

        skip = lock.synchronize { feed_checksums[rss_url] == checksum }
        if skip
          LOGGER.debug "Skipping #{source_title} (feed unchanged)"
          next
        end

        feed = RSS::Parser.parse(rss_content, false)

        feed.items.each do |item|
          link_url = item_link_url(item)
          if link_url.to_s.strip.empty?
            stats[:missing_link] += 1
            next
          end

          title_text = item_title_text(item).to_s.strip
          if title_text.empty?
            stats[:missing_title] += 1
            next
          end
          published_time = published_at(item)
          unless published_time
            stats[:missing_publish_date] += 1
            next
          end

          if stale_item?(published_time)
            stats[:stale] += 1
            next
          end

          original_html = item_content_html(item).to_s.strip
          original_html = fetch_article_body_html(item.link).to_s.strip if original_html.empty? && item.link

          if original_html.empty?
            stats[:empty_content] += 1
            next
          end

          existing_file = Dir.glob("#{NEWS_DIR}/*.md").find do |f|
            fm = read_frontmatter(f)
            fm['original_content'] && fm['source_url'].to_s == link_url.to_s
          end
          if existing_file
            stats[:duplicates] += 1
            next
          end

          content_md = ReverseMarkdown.convert(original_html)

          date_prefix = published_time.strftime('%Y-%m-%d')
          title_slug = filename_slug(title_text, link_url, date_prefix)
          filename = "#{NEWS_DIR}/#{date_prefix}-#{title_slug}.md"

          news_frontmatter = {
            'title' => title_text,
            'date' => published_time.iso8601,
            'source' => source_title,
            'source_url' => link_url.to_s,
            'original_content' => original_html
          }

          File.write(filename, "---\n#{news_frontmatter.to_yaml.sub(/\A---\s*\n/, '')}---\n\n#{content_md}")
          stats[:created] += 1
        end

        lock.synchronize { feed_checksums[rss_url] = checksum }
        LOGGER.info feed_summary_line(source_title, rss_url, stats)
      rescue OpenURI::HTTPError, SocketError, Net::OpenTimeout, Net::ReadTimeout, Timeout::Error => e
        LOGGER.error "Failed to fetch RSS feed for source '#{source_title}' (#{rss_url}): #{e.message}"
        next
      rescue OpenSSL::SSL::SSLError => e
        LOGGER.error "SSL error for source '#{source_title}' (#{rss_url}): #{e.message}"
        next
      rescue RSS::NotWellFormedError => e
        LOGGER.error "Failed to parse RSS feed for source '#{source_title}' (#{rss_url}): #{e.message}"
        next
      end
    end
  end
end

workers.each(&:join)
save_feed_checksums(feed_checksums)
