#!/usr/bin/env ruby
# frozen_string_literal: true

require 'yaml'
require 'rss'
require 'open-uri'
require 'openssl'
require 'fileutils'
require 'time'
require 'digest'
require 'reverse_markdown'
require 'nokogiri'

NEWS_DIR = '_posts'
SOURCES_DIR = '_organizations'
MAX_ITEM_AGE_DAYS = 365
MAX_FILENAME_BYTES = 255
CHECKSUM_FILE = File.expand_path('feed_checksums.yml', __dir__)

FileUtils.mkdir_p(NEWS_DIR)

# Helper to read frontmatter from a Markdown file
def read_frontmatter(file_path)
  content = File.read(file_path)
  if content =~ /\A---\s*\n(.*?\n?)^---\s*$\n?/m
    YAML.safe_load(Regexp.last_match(1)) || {}
  else
    {}
  end
end

ARTICLE_BODY_SELECTORS = [
  '#news_content_body',
  '[id*="news_content_body"]',
  '.news_content_body',
  '[class*="news_content_body"]',
  '.news-body',
  '.article-body',
  '.article__body',
  '.news-article__body',
  'article .body'
].freeze

def fetch_article_body_html(url)
  html = URI.open(url).read
  doc = Nokogiri::HTML(html)

  ARTICLE_BODY_SELECTORS.each do |selector|
    node = doc.at_css(selector)
    next unless node

    snippet = node.inner_html.to_s.strip
    return snippet unless snippet.empty?
  end

  fallback = doc.at_css('main') || doc.at_css('#main') || doc.at_css('#content')
  fallback&.inner_html&.strip
rescue OpenURI::HTTPError, OpenSSL::SSL::SSLError, SocketError => e
  warn "Failed to fetch article body (#{url}): #{e.message}"
  nil
rescue StandardError => e
  warn "Unexpected error scraping #{url}: #{e.message}"
  nil
end

def published_at(item)
  candidates = []
  candidates << item.pubDate if item.respond_to?(:pubDate)
  candidates << item.dc_date if item.respond_to?(:dc_date)
  candidates << item.updated if item.respond_to?(:updated)
  candidates << item.date if item.respond_to?(:date)

  value = candidates.compact.first
  return value if value.is_a?(Time)

  if value.respond_to?(:to_time)
    value.to_time
  elsif value
    Time.parse(value.to_s)
  end
rescue StandardError
  nil
end

def stale_item?(published_time)
  cutoff = Time.now - (MAX_ITEM_AGE_DAYS * 24 * 60 * 60)
  published_time < cutoff
end

def sanitized_slug(text)
  text.to_s.downcase.gsub(/[^a-z0-9]+/, '-').gsub(/^-|-$/, '')
end

def filename_slug(title, link, date_prefix)
  base_slug = sanitized_slug(title)
  fallback_slug = Digest::SHA1.hexdigest(link.to_s)[0, 12]
  base_slug = fallback_slug if base_slug.empty?

  max_slug_length = MAX_FILENAME_BYTES - "#{date_prefix}-".length - '.md'.length
  max_slug_length = 1 if max_slug_length < 1

  return base_slug if base_slug.length <= max_slug_length

  digest = Digest::SHA1.hexdigest(link.to_s)[0, 8]
  truncated_length = max_slug_length - digest.length - 1
  truncated_length = 0 if truncated_length.negative?
  truncated = base_slug[0, truncated_length]
  slug = [truncated, digest].reject(&:empty?).join('-')
  slug = fallback_slug if slug.empty?
  slug.length > max_slug_length ? slug[0, max_slug_length] : slug
end

def load_feed_checksums
  return {} unless File.exist?(CHECKSUM_FILE)

  data = YAML.safe_load(File.read(CHECKSUM_FILE))
  data.is_a?(Hash) ? data : {}
rescue StandardError => e
  warn "Failed to load feed checksum cache (#{CHECKSUM_FILE}): #{e.message}"
  {}
end

def save_feed_checksums(data)
  File.write(CHECKSUM_FILE, data.to_yaml)
end

feed_checksums = load_feed_checksums

# Iterate through all Markdown files in _sources
Dir.glob("#{SOURCES_DIR}/*.md") do |source_file|
  frontmatter = read_frontmatter(source_file)
  rss_url = frontmatter['news_rss_url']
  source_title = frontmatter['title']

  next unless rss_url # Skip if no RSS URL

  begin
    rss_content = URI.open(rss_url, &:read)
    checksum = Digest::SHA256.hexdigest(rss_content)

    if feed_checksums[rss_url] == checksum
      puts "Skipping #{source_title} (feed unchanged)"
      next
    end

    feed = RSS::Parser.parse(rss_content, false)

    feed.items.each do |item|
        published_time = published_at(item)
        unless published_time
          warn "Skipping #{item.link || item.title} (missing publish date)"
          next
        end

        if stale_item?(published_time)
          puts "Skipping #{item.link || item.title} (older than #{MAX_ITEM_AGE_DAYS} days)"
          next
        end

        # Get original HTML content from RSS item
        original_html = item.respond_to?(:content_encoded) && item.content_encoded ? item.content_encoded : item.description
        original_html = original_html.to_s.strip
        original_html = fetch_article_body_html(item.link).to_s.strip if original_html.empty? && item.link

        if original_html.empty?
          warn "No content available for #{item.link}, skipping."
          next
        end

        # Skip if we've already imported this article (has original_content)
        existing_file = Dir.glob("#{NEWS_DIR}/*.md").find do |f|
          fm = read_frontmatter(f)
          fm['original_content'] && fm['source_url'] == item.link
        end
        if existing_file
          puts "Skipping #{item.title} (already imported)"
          next
        end

        # Convert HTML to Markdown for content body
        content_md = ReverseMarkdown.convert(original_html)

        # Generate a safe filename
        date_prefix = published_time.strftime('%Y-%m-%d')
        title_slug = filename_slug(item.title, item.link, date_prefix)
        filename = "#{NEWS_DIR}/#{date_prefix}-#{title_slug}.md"

        # Build frontmatter
        news_frontmatter = {
          'title' => item.title,
          'date' => published_time.iso8601,
          'source' => source_title,
          'source_url' => item.link,
          'original_content' => original_html
        }

        # Write the news file with proper single frontmatter
        File.write(filename, "---\n#{news_frontmatter.to_yaml.sub(/\A---\s*\n/, '')}---\n\n#{content_md}")
        puts "Created #{filename}"
      end

    feed_checksums[rss_url] = checksum
  rescue OpenURI::HTTPError, SocketError => e
    warn "Failed to fetch RSS feed for source '#{source_title}' (#{rss_url}): #{e.message}"
    next
  rescue OpenSSL::SSL::SSLError => e
    warn "SSL error for source '#{source_title}' (#{rss_url}): #{e.message}"
    next
  rescue RSS::NotWellFormedError => e
    warn "Failed to parse RSS feed for source '#{source_title}' (#{rss_url}): #{e.message}"
    next
  end
end

save_feed_checksums(feed_checksums)
