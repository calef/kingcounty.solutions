#!/usr/bin/env ruby
# frozen_string_literal: true

require 'json'
require 'yaml'
require 'fileutils'
require 'ruby/openai'
require 'open-uri'
require 'nokogiri'
require_relative '../lib/logging'

LOGGER = Logging.build_logger(env_var: 'LOG_LEVEL')

# Configure Ruby-OpenAI client
client = OpenAI::Client.new(access_token: ENV.fetch('OPENAI_API_KEY'))

NEWS_DIR = '_posts'
TOPIC_DIR = '_topics'
MAX_ARTICLE_CHARS = 20_000
OPENAI_MODEL = ENV.fetch('OPENAI_MODEL', 'gpt-4o-mini')
OPENAI_TOPIC_MODEL = ENV.fetch('OPENAI_TOPIC_MODEL', OPENAI_MODEL)

def fetch_article_text(url)
  html = URI.open(url, open_timeout: 10, read_timeout: 15).read
  doc = Nokogiri::HTML(html)
  # Remove scripts, styles, and navigation cruft
  doc.search('script, style, nav, header, footer, noscript, iframe').remove
  text = doc.css('article, main, body').text
  text.strip.gsub(/\s+/, ' ')
rescue StandardError => e
  LOGGER.warn "Error fetching #{url}: #{e.class} - #{e.message}"
  nil
end

def read_front_matter(content)
  return [nil, nil] unless content =~ /\A---\s*\n(.*?)\n---\s*\n/m

  [YAML.safe_load(Regexp.last_match(1)) || {}, Regexp.last_match.post_match]
end

def default_title(path)
  File.basename(path, '.md').tr('-', ' ')
end

def load_topics
  Dir.glob("#{TOPIC_DIR}/*.md").sort.filter_map do |path|
    content = File.read(path)
    front_matter, body = read_front_matter(content)
    next unless front_matter

    title = front_matter['title'] || default_title(path)
    summary = (body || '').strip
    { 'title' => title, 'summary' => summary }
  end
end

def classify_topics(client, text, topics)
  return [] if text.to_s.strip.empty? || topics.empty?

  catalog_lines = topics.map do |topic|
    summary = topic['summary']
    "- #{topic['title']}: #{summary&.empty? ? 'No summary provided.' : summary}"
  end
  allowed_titles = topics.map { |topic| topic['title'] }

  prompt = <<~PROMPT
    You are selecting topics for a local news post.

    Topic catalog:
    #{catalog_lines.join("\n")}

    News content:
    #{text.strip}

    Return a JSON array of topic titles from the catalog above that clearly apply to this news post.
    Only use titles from the catalog; do not invent new topics.
    Exclude topics that are only weakly related or unclear.
  PROMPT

  attempts = 0
  while attempts < 3
    attempts += 1
    begin
      response = client.chat(
        parameters: {
          model: OPENAI_TOPIC_MODEL,
          messages: [
            { role: 'system', content: 'You are a precise classification assistant who responds with JSON arrays.' },
            { role: 'user', content: prompt }
          ],
          temperature: 0.2
        }
      )

      content = response.dig('choices', 0, 'message', 'content')
      next unless content

      cleaned = content.gsub(/\A```json\s*/i, '').gsub(/```\s*\z/, '')
      parsed = JSON.parse(cleaned)
      selected = Array(parsed).map(&:to_s).select { |title| allowed_titles.include?(title) }.uniq
      return selected
    rescue Faraday::TooManyRequestsError
      LOGGER.warn "Rate limited during topic classification, waiting 5 seconds before retry (attempt #{attempts})"
      sleep 5
    rescue JSON::ParserError
      LOGGER.warn "Non-JSON response while classifying topics: #{content.inspect}"
      next
    end

  end

  []
end

topics = load_topics
stats = Hash.new(0)

Dir.glob("#{NEWS_DIR}/*.md").each do |file_path|
  content = File.read(file_path)

  front_matter, body = read_front_matter(content)
  unless front_matter
    LOGGER.warn "Skipping #{file_path}: no front matter"
    stats[:skipped_no_frontmatter] += 1
    next
  end

  if front_matter['published'] == false
    LOGGER.debug "Skipping #{file_path}: published is false"
    stats[:skipped_unpublished] += 1
    next
  end

  needs_summary = front_matter['summarized'] != true
  needs_topics = Array(front_matter['topics']).empty?
  next unless needs_summary || needs_topics

  source_url = front_matter['source_url']
  if needs_summary && source_url.nil?
    LOGGER.warn "Skipping #{file_path}: no source_url"
    stats[:skipped_missing_source] += 1
    next
  end

  article_text =
    if source_url
      fetch_article_text(source_url)
    else
      body
    end
  article_text ||= body # fallback to the original content if fetch failed

  if article_text&.length.to_i > MAX_ARTICLE_CHARS
    LOGGER.info "Truncating #{file_path} article text from #{article_text.length} to #{MAX_ARTICLE_CHARS} chars"
    article_text = article_text[0, MAX_ARTICLE_CHARS]
  end

  summary_text = needs_summary ? nil : (body&.strip || article_text&.strip)

  prompt = <<~PROMPT
    Summarize the following article in 200 words or less in Markdown format for a new aggregator blog.

    Article URL: #{source_url}

    In the summary:
      1. Do not include a link back to the source URL.
      2. Do not include an image if one is referenced in the text.
      3. Do not include any commentary or explanation about this process.
      4. Focus only on the provided text (do not mention if the content was truncated).
      5. Do not include any headings or code blocks.
      6. Do not write that the article says something, just write what the article says. Do not write "The article discusses..." or "The article outlines...". Do write a summary of the article content.

    ARTICLE CONTENT:
    #{article_text}
  PROMPT

  begin
    # Retry logic: up to 3 attempts in case of rate limits or temporary failures
    attempts = 0
    while needs_summary && attempts < 3 && summary_text.nil?
      attempts += 1
      begin
        response = client.chat(
          parameters: {
            model: OPENAI_MODEL,
            messages: [
              { role: 'system', content: 'You are a helpful assistant.' },
              { role: 'user', content: prompt }
            ],
            temperature: 0.7
          }
        )
        if (error_message = response.dig('error', 'message'))
          LOGGER.warn "OpenAI error for #{file_path}: #{error_message}"
          break
        end
        summary_text = response.dig('choices', 0, 'message', 'content')&.strip
      rescue Faraday::TooManyRequestsError
        LOGGER.warn "Rate limited, waiting 5 seconds before retry (attempt #{attempts})"
        sleep 5
      end
    end

    if needs_summary && (summary_text.nil? || summary_text.empty?)
      LOGGER.warn "Skipped #{file_path}: could not summarize"
      stats[:failed_summary] += 1
      next
    end

    front_matter['original_markdown_body'] ||= body&.strip if needs_summary
    front_matter['summarized'] = true if needs_summary
    summary_text ||= body&.strip || ''

    if needs_topics
      classified_topics = classify_topics(client, summary_text, topics)
      front_matter['topics'] = classified_topics
      if classified_topics.empty?
        LOGGER.info "No topics matched for #{file_path}"
        stats[:missing_topics] += 1
      end
    end

    if needs_topics && Array(front_matter['topics']).empty?
      front_matter['published'] = false
    end

    yaml_front_matter = YAML.dump(front_matter).sub(/\A---\s*\n/, '').strip
    updated_content = ['---', yaml_front_matter, '---', '', summary_text].join("\n")
    File.write(file_path, updated_content)
    stats[:updated] += 1
    LOGGER.info "Updated #{file_path}"
  rescue StandardError => e
    stats[:errors] += 1
    LOGGER.error "Error processing #{file_path}: #{e.class} - #{e.message}"
  end
end

summary_fields = {
  updated: stats[:updated],
  skipped_no_frontmatter: stats[:skipped_no_frontmatter],
  skipped_unpublished: stats[:skipped_unpublished],
  skipped_missing_source: stats[:skipped_missing_source],
  failed_summary: stats[:failed_summary],
  missing_topics: stats[:missing_topics],
  errors: stats[:errors]
}

summary_text = summary_fields.map { |key, value| "#{key}=#{value}" }.join(', ')
LOGGER.info "summarize-news complete: #{summary_text}"
