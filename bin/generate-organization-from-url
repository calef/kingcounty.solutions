#!/usr/bin/env ruby
# frozen_string_literal: true

require 'fileutils'
require 'json'
require 'nokogiri'
require 'open-uri'
require 'ruby/openai'
require 'uri'
require 'yaml'

ORG_DIR = '_organizations'
TOPIC_DIR = '_topics'
PLACE_DIR = '_places'
DEFAULT_TYPE = 'Community-Based Organization'
MAX_PAGES = Integer(ENV.fetch('ORG_SCRAPER_MAX_PAGES', 5))
PAGE_SNIPPET = Integer(ENV.fetch('ORG_SCRAPER_PAGE_SNIPPET', 3000))
READ_TIMEOUT = Integer(ENV.fetch('ORG_SCRAPER_TIMEOUT', 10))
OPENAI_MODEL = ENV.fetch('OPENAI_ORG_MODEL', 'gpt-4o-mini')

def normalize_url(url)
  uri = URI(url)
  uri.fragment = nil
  uri.to_s.sub(%r{/$}, '').downcase
rescue URI::InvalidURIError
  url.to_s.strip.downcase.sub(%r{/$}, '')
end

def canonical_url(url)
  uri = URI(url)
  uri.scheme ||= 'https'
  uri.fragment = nil
  uri.to_s
rescue URI::InvalidURIError
  url
end

def load_existing_websites
  Dir.glob("#{ORG_DIR}/*.md").each_with_object(Set.new) do |path, set|
    content = File.read(path)
    next unless content =~ /^website:\s*(.+)$/

    set << normalize_url(Regexp.last_match(1).strip)
  end
end

def load_existing_types
  Dir.glob("#{ORG_DIR}/*.md").each_with_object(Set.new) do |path, set|
    content = File.read(path)
    next unless content =~ /^type:\s*(.+)$/

    set << Regexp.last_match(1).strip
  end
end

def fetch_page(url)
  html = URI.open(url, open_timeout: READ_TIMEOUT, read_timeout: READ_TIMEOUT).read
  Nokogiri::HTML(html)
rescue StandardError => e
  warn "Skipping #{url}: #{e.class} #{e.message}"
  nil
end

def extract_text(doc)
  cleaned = doc.dup
  cleaned.search('script, style, nav, header, footer, noscript, iframe').remove
  cleaned.text.gsub(/\s+/, ' ').strip
end

def gather_pages(start_url)
  start_uri = URI(canonical_url(start_url))
  queue = [start_uri]
  seen = Set.new
  pages = []

  until queue.empty? || pages.size >= MAX_PAGES
    uri = queue.shift
    normalized = normalize_url(uri.to_s)
    next if seen.include?(normalized)

    seen << normalized
    doc = fetch_page(uri.to_s)
    next unless doc

    pages << { url: uri.to_s, text: extract_text(doc), doc: doc }

    doc.css('a[href]').map { |a| a['href'] }.compact.each do |href|
      begin
        link = URI.join(uri, href)
      rescue StandardError
        next
      end
      next unless link.host == start_uri.host

      normalized_link = normalize_url(link.to_s)
      next if seen.include?(normalized_link)
      next if queue.any? { |queued| normalize_url(queued.to_s) == normalized_link }

      queue << link
    end
  end

  pages
end

def load_topics
  Dir.glob("#{TOPIC_DIR}/*.md").filter_map do |path|
    content = File.read(path)
    next unless content =~ /\A---\s*\n(.*?)\n---/m

    data = YAML.safe_load(Regexp.last_match(1))
    data['title']
  end.compact.sort
end

def load_place_titles
  Dir.glob("#{PLACE_DIR}/*.md").filter_map do |path|
    content = File.read(path)
    next unless content =~ /\A---\s*\n(.*?)\n---/m

    data = YAML.safe_load(Regexp.last_match(1)) || {}
    data['title'] || File.basename(path, '.md').tr('-', ' ')
  end.compact.sort
end

def openai_client
  OpenAI::Client.new(access_token: ENV.fetch('OPENAI_API_KEY'))
end

def build_prompt(url, pages, topics, types)
  snippet = pages.map { |page| [page[:url], page[:text][0, PAGE_SNIPPET]].join("\n") }.join("\n\n")
  <<~PROMPT
    You are creating metadata for a local organization for a community resource directory.
    Only use the provided content. If unsure about a field, omit it or leave it empty.

    Allowed topic titles: #{topics.join('; ')}
    Allowed organization types: #{types.join('; ')}

    Return a JSON object with keys:
      - title (string, required)
      - type (string chosen from the allowed organization types)
      - acronym (string of capital letters like YMCA; omit if none)
      - jurisdictions (array of geographic place names mentioned in the content; omit if unclear)
      - topics (array of titles chosen from the allowed topics list, most relevant only)
      - parent_organization (string or null)
      - news_rss_url (string or null)
      - events_ical_url (string or null)
      - phone (string or null)
      - email (string or null)
      - address (string or null)
      - summary (markdown summary of services, maximum 100 words)

    Website to set: #{url}

    Scraped content:
    #{snippet}
  PROMPT
end

def parse_response(content)
  cleaned = content.to_s.gsub(/\A```json\s*/i, '').gsub(/```\s*\z/, '')
  JSON.parse(cleaned)
rescue JSON::ParserError
  nil
end

def key_value_or_nil(hash, key)
  value = hash[key]
  return nil if value.nil?

  if value.is_a?(String)
    trimmed = value.strip
    return nil if trimmed.empty?

    trimmed
  elsif value.respond_to?(:empty?) && value.empty?
    nil
  else
    value
  end
end

def slugify(title)
  base = title.to_s.downcase.gsub(/[^a-z0-9]+/, '-').gsub(/^-|-$/, '')
  base.empty? ? 'organization' : base
end

def ensure_unique_slug(base)
  slug = base
  idx = 1
  while File.exist?(File.join(ORG_DIR, "#{slug}.md"))
    slug = "#{base}-#{idx}"
    idx += 1
  end
  slug
end

def enforce_type(value, allowed)
  return nil if value.nil?

  allowed.find { |t| t.casecmp(value.to_s.strip).zero? }
end

def extract_feed_links(pages)
  rss = nil
  ical = nil
  pages.each do |page|
    doc = page[:doc]
    base = page[:url]
    next unless doc

    doc.css('link[rel="alternate"]').each do |link|
      href = link['href']
      type = link['type'].to_s.downcase
      next unless href

      full = begin
        URI.join(base, href).to_s
      rescue StandardError
        next
      end
      rss ||= full if type.include?('rss') || type.include?('atom') || href =~ /(rss|atom|feed)/i
      ical ||= full if type.include?('calendar') || href =~ /\.ics(\?|$)/i
    end

    doc.css('a[href]').each do |a|
      href = a['href']
      next unless href

      full = begin
        URI.join(base, href).to_s
      rescue StandardError
        next
      end
      ical ||= full if href =~ /\.ics(\?|$)/i
    end
  end
  [rss, ical]
end

def write_organization_file(slug, front_matter, body)
  FileUtils.mkdir_p(ORG_DIR)
  path = File.join(ORG_DIR, "#{slug}.md")

  ordered_keys = %w[acronym jurisdictions news_rss_url events_ical_url parent_organization phone email address title
                    topics type website]
  ordered = {}
  ordered_keys.each do |key|
    ordered[key] = front_matter[key] if front_matter.key?(key)
  end

  yaml = YAML.dump(ordered).sub(/\A---\s*\n/, '')
  File.write(path, "---\n#{yaml}---\n\n#{body.strip}\n")
  path
end

def main
  abort "Usage: #{File.basename($PROGRAM_NAME)} URL" if ARGV.empty?

  raw_url = ARGV.first
  website_url = canonical_url(raw_url)
  normalized = normalize_url(website_url)

  existing_websites = load_existing_websites
  existing_types = load_existing_types.to_a.sort
  if existing_websites.include?(normalized)
    warn "Organization with website #{website_url} already exists; skipping."
    return
  end

  pages = gather_pages(website_url)
  abort "No content scraped from #{website_url}" if pages.empty?

  rss_candidate, ical_candidate = extract_feed_links(pages)
  topics = load_topics
  places = load_place_titles
  types = existing_types.empty? ? [DEFAULT_TYPE] : existing_types
  prompt = build_prompt(website_url, pages, topics, types)

  client = openai_client
  response = client.chat(
    parameters: {
      model: OPENAI_MODEL,
      temperature: 0.2,
      messages: [
        { role: 'system', content: 'You are a concise metadata extraction bot.' },
        { role: 'user', content: prompt }
      ]
    }
  )

  content = response.dig('choices', 0, 'message', 'content')
  data = parse_response(content) || {}

  title = key_value_or_nil(data, 'title') || URI(website_url).host
  slug = ensure_unique_slug(slugify(title))

  front_matter = {}
  %w[acronym jurisdictions news_rss_url events_ical_url parent_organization phone email address topics
     type].each do |key|
    value = key_value_or_nil(data, key)
    front_matter[key] = value unless value.nil?
  end

  acronym = front_matter['acronym']
  front_matter['acronym'] = nil unless acronym&.match?(/\A[A-Z0-9&]{2,10}\z/)
  front_matter.compact!

  if (juris = front_matter['jurisdictions'])
    allowed = places.to_set
    filtered = Array(juris).map(&:to_s).map(&:strip).reject(&:empty?).select { |j| allowed.include?(j) }.uniq
    front_matter['jurisdictions'] = if filtered.empty?
                                      ['King County']
                                    else
                                      filtered
                                    end
  end

  if (type_value = front_matter['type'])
    coerced = enforce_type(type_value, types)
    front_matter['type'] = coerced || DEFAULT_TYPE
  else
    front_matter['type'] = DEFAULT_TYPE
  end

  front_matter['news_rss_url'] = rss_candidate if rss_candidate && !front_matter.key?('news_rss_url')
  front_matter['events_ical_url'] = ical_candidate if ical_candidate && !front_matter.key?('events_ical_url')

  front_matter['title'] = title
  front_matter['website'] = website_url

  body = key_value_or_nil(data, 'summary') || 'Description forthcoming.'
  word_limited = body.split(/\s+/)[0, 100].join(' ').strip
  body = word_limited.empty? ? 'Description forthcoming.' : word_limited

  path = write_organization_file(slug, front_matter, body)
  puts "Created #{path}"
end

main if $PROGRAM_NAME == __FILE__
