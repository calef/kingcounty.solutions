---
date: '2025-11-10T21:17:00+00:00'
images: []
original_content: |-
  <div class="is-layout-constrained wp-block-group"><div class="wp-block-group__inner-container">
  <p><strong>A note about AI: </strong><em>On the Talkspace blog we aim to provide trustworthy coverage of all the mental health topics people might be curious about, by delivering science-backed, clinician-reviewed information. Our articles on artificial intelligence (AI) and how this emerging technology may intersect with mental health and healthcare are designed to educate and add insights to this cultural conversation. We believe that therapy, at its core, is focused around the therapeutic connection between human therapists and our members. At Talkspace we only use ethical and responsible AI tools that are developed in partnership with our human clinicians. These tools aren’t designed to replace qualified therapists, but to enhance their ability to keep delivering high-quality care.</em> <em>To learn more, visit our </em><a href="https://www.talkspace.com/ai-at-talkspace" target="_blank" rel="noopener"><em>AI-supported therapy page</em></a><em>.</em></p>



  <div style="height:30px" aria-hidden="true" class="wp-block-spacer"></div>



  <p>Artificial intelligence (AI) tools, like ChatGPT, are becoming part of our daily lives. Many people use ChatGPT as an assistant to help brainstorm ideas, draft emails, or answer questions quickly. The fast and helpful responses can feel magical, making it tempting to blindly trust the information. What happens when ChatGPT gives you an answer that’s completely wrong?</p>



  <p>The algorithms that power ChatGPT can sometimes produce false or misleading information that sounds convincing. This is called an AI hallucination. ChatGPT hallucinations can be dangerous if they influence your beliefs, emotions, or decisions. Continue reading to learn more about what causes ChatGPT hallucinations, why they’re harmful, and how to protect your mental well-being while using AI tools, like ChatGPT.&nbsp;</p>



  <h2>What Are AI “Hallucinations”?</h2>



  <p>AI “hallucinations&#8221; happen when a ChatGPT bot produces information that sounds plausible, but it’s actually false, misleading, or unverifiable.&nbsp;</p>



  <p>Experts categorize hallucinations into two different types:</p>



  <ul>
  <li><strong>Intrinsic hallucinations: </strong>When the response generated by ChatGPT misrepresents or distorts information. For example, a chatbot may summarize a medical study in a way that changes its meaning or significance.&nbsp;</li>



  <li><strong>Extrinsic hallucinations: </strong>When ChatGPT generates information that isn’t backed up by real-world facts. For example, ChatGPT might invent a research study or statistic that doesn’t exist. An extrinsic hallucination doesn’t necessarily have to be wrong. However, the information isn’t verifiable from any known source or reference.&nbsp;</li>
  </ul>



  <p>It can be difficult to spot a ChatGPT hallucination example because of how the response is delivered. ChatGPT hallucinations aren’t simple mistakes, like a typo. It’s an answer that seems polished, confident, and authoritative. We’re conditioned to think information is more persuasive when it’s written in this tone. Understanding what a ChatGPT hallucination is and knowing they can happen even when it sounds trustworthy is important to keep yourself safe when using this new technology.&nbsp;</p>



  <blockquote class="wp-block-quote">
  <p>“AI hallucinations can feel convincing because they exploit our brain’s tendency to trust confident, fluent, and human-like language—even when the content is entirely false.”</p>
  <cite>&#8211; <a href="https://www.talkspace.com/mental-health/conditions/author/cynthia-v-catchings/" target="_blank" rel="noreferrer noopener">Talkspace therapist Cynthia Catchings, PhD, LCSW-S</a></cite></blockquote>



  <h2>What Causes ChatGPT to Hallucinate?</h2>



  <p>When you ask ChatGPT a question, the bot doesn’t think or fact-check the way a person does. Instead, it generates a response by predicting the most likely answer based on patterns it learned from being fed large amounts of text. While this process can often produce accurate answers, it’s not perfect and can lead to mistakes.&nbsp;</p>



  <p>These mistakes can happen for a number of different reasons. ChatGPT is trained on both reliable and unreliable information. That means that errors in the information it was trained on can also show up in the answers. Another reason for ChatGPT hallucinations is that when it doesn&#8217;t know an answer, it may “guess,” resulting in answers that sound real, but aren’t. ChatGPT prioritizes a natural-sounding response, even when the information isn’t accurate.&nbsp;</p>



  <h2>How Often Does ChatGPT Hallucinate?</h2>



  <p>ChatGPT can be impressive, but it makes mistakes more than you may realize. Tests done by OpenAI (the creator of ChatGPT) <a target="_blank" rel="noopener" href="https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf">found that</a> the ChatGPT hallucination rate was between 33% and 79%, depending on the type of test and the model used. OpenAI says the newer models hallucinate less often, but no AI model is completely free of this problem.&nbsp;</p>



  <h2>When AI Hallucinations Become Dangerous</h2>



  <p>In some cases, you might be able to brush off a ChatGPT hallucination as a quirky mistake. However, when you begin to rely on ChatGPT for more serious information, hallucinations can have a bigger effect. False or misleading answers can affect decisions, health, and how you think.&nbsp;</p>



  <h3>Misinformation and poor decisions</h3>



  <p>If you assume ChatGPT is always reliable, you may use it to guide major decisions that affect your life.&nbsp;</p>



  <p>For example, when using ChatGPT for health advice, acting on a hallucination could lead you to take actions that worsen your illness or delay you getting the treatment you need.&nbsp;&nbsp;</p>



  <p>Asking ChatGPT about how to invest your money or file your taxes could result in incorrect information that directly impacts your finances.&nbsp;</p>



  <p>Using ChatGPT in legal scenarios can also have major implications. There have already been cases where AI hallucinations have shown up in legal cases by citing cases that don’t exist. If you’re relying on this information to defend your case in court, there could be serious consequences.&nbsp;</p>



  <h3>Mental health impacts</h3>



  <p>ChatGPT hallucinations can also affect your mental health. If you get an answer from a chatbot that’s unexpected or contradicts what you know, it can cause anxiety or confusion. You might find yourself questioning your own memory or judgment, wondering if you’re the one who’s misunderstanding some of the information provided.&nbsp;</p>



  <blockquote class="wp-block-quote">
  <p>“People may recognize that AI use is starting to negatively impact their thinking or emotional state when they feel anxious, dependent, or detached from real-life interactions, or when they start trusting AI responses over their own intuition or human relationships.”</p>
  <cite>&#8211; <a href="https://www.talkspace.com/mental-health/conditions/author/cynthia-v-catchings/" target="_blank" rel="noreferrer noopener">Talkspace therapist Cynthia Catchings, PhD, LCSW-S</a></cite></blockquote>



  <p>There have also been emerging reports of <a href="https://www.talkspace.com/blog/ai-psychosis/" target="_blank" rel="noreferrer noopener">AI-induced psychosis</a> where reliance on AI tools contributes to losing touch with reality. Since AI is still new, researchers don’t know the full extent to which this new technology might affect mental health.&nbsp;</p>



  <h3>Overreliance and loss of critical thinking</h3>



  <p>When you start using ChatGPT and start getting confident, polished responses, it’s easy to rely on this information more and more. You may have even started by double-checking the system’s responses for accuracy. If you’ve found the information to be accurate in the past, you may stop fact-checking or questioning the responses in the future. Over time, this can weaken your own critical thinking and decision-making skills.&nbsp;</p>



  <p>A <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08872">2025 study</a> found that students who used ChatGPT to write an essay had lower brain activity and underperformed in their tasks compared to students who didn’t use AI tools. Although ChatGPT can be convenient, it may come at the cost of accuracy and your critical thinking skills.&nbsp;</p>



  <h3>Dependency on AI for reassurance</h3>



  <p>If you’ve found ChatGPT to be helpful in completing your daily tasks, you may also wonder about its ability to provide reassurance and emotional support. Some people are turning to <a target="_blank" rel="noopener" href="https://www.talkspace.com/blog/ai-mental-health/">AI for mental health support</a>, even going as far as using <a target="_blank" rel="noopener" href="https://www.talkspace.com/blog/chatgpt-therapist/">ChatGPT as a therapist</a>.&nbsp;</p>



  <p>Depending on ChatGPT to help validate your feelings or guide personal decisions may reduce your confidence in your own judgment and that of your loved ones whom you normally confide in. It’s important to remember that while ChatGPT can be a helpful sounding board, it’s not a replacement for real human connection.&nbsp;</p>



  <h2>Who Is Most at Risk?</h2>



  <p>Anyone who uses ChatGPT may encounter a hallucination. If your prompt or the question you asked is low-stakes, a ChatGPT hallucination may present an inconvenience. However, in high-stakes situations that involve your health or safety, a hallucination can pose a big risk.&nbsp;</p>



  <p>People who spend significant time engaging with ChatGPT may have a higher risk of encountering a hallucination that causes harm. For example, people who turn to ChatGPT for companionship or emotional validation may be at risk for more harm from a hallucination.&nbsp;</p>



  <p>Another group of people who may be more vulnerable to harm from ChatGPT hallucinations is those with preexisting mental health conditions, like anxiety or psychosis. Someone who already has a mental health condition may become confused or distressed by an incorrect or misleading AI hallucination.&nbsp;</p>



  <h2>How To Use ChatGPT Responsibly &amp; Avoid Risks</h2>



  <p>ChatGPT can be a helpful tool when it’s used responsibly. It’s important to remember that no matter how advanced the technology is, it’s not perfect, and it can make mistakes. That doesn’t mean you shouldn’t use ChatGPT, but you should be aware of some ways to help reduce your risk of harm from ChatGPT hallucinations.&nbsp;</p>



  <h3>Verify information</h3>



  <p>The best way to stay safe when using ChatGPT is to double-check the information it gives you. Even when the answers sound confident, be aware that they could be incomplete, misleading, or outright wrong in some cases.&nbsp;</p>



  <p>Cross-check the answer with credible sources before making any decisions. Look for consistency with trusted professional advice. For financial guidance, consult official government resources or a licensed financial advisor. If you have legal questions, your best source is a qualified attorney.&nbsp;</p>



  <p>To help verify health information, you can check with trusted health sources, such as those from large hospitals or the National Institutes of Health. Even after checking with trusted sources, it’s important to talk to a healthcare provider if you have any concerns.&nbsp;</p>



  <p>It’s important to understand that extrinsic AI hallucinations may not be verifiable. If you can’t find information to back up an answer from ChatGPT, it may be a hallucination.&nbsp;</p>



  <h3>Set boundaries on use</h3>



  <p>Spending too much time using ChatGPT can put you at risk of harm from a ChatGPT hallucination. To help avoid this, limit prolonged sessions and try to avoid late-night sessions where fatigue can make it harder to think critically.&nbsp;</p>



  <p>It’s also important to pay attention to how your interactions with ChatGPT affect your mood and how you think. If you notice that you’re feeling more anxiety, self-doubt, or frustration, it may be a sign that you need to take a step back from your ChatGPT use.&nbsp;</p>



  <h3>Treat AI as a supplement, not a substitute</h3>



  <p>ChatGPT is a great tool when it’s used for things like brainstorming or as a support to your work. However, the algorithms that power ChatGPT and similar AI models aren’t a replacement for real human expertise shaped over years of experience. If you’re planning on making important decisions, make sure to keep human oversight central to your decision-making process.&nbsp;</p>



  <h2>Leaning on Humans for Support</h2>



  <p>ChatGPT and other types of AI can be helpful tools, but they’re not a substitute for real human guidance. If you ever feel like your use of ChatGPT is leaving you anxious, confused, or unable to trust your own judgment, it may be a sign to reach out for professional guidance. A licensed therapist can give you coping strategies, emotional support, and a human perspective that ChatGPT simply can’t provide.&nbsp;</p>



  <p>When it comes to the use of <a href="https://www.talkspace.com/ai-at-talkspace" target="_blank" rel="noopener">AI at Talkspace</a>, these tools are used to enhance human-first care, not replace it. With <a data-wpil="url" href="https://www.talkspace.com/online-therapy/" target="_blank" rel="noopener">online therapy</a>, you can connect with a licensed therapist who can help you navigate the challenges of daily life and prioritize your well-being. With Talkspace, you can turn to a real person using the best <a href="https://www.talkspace.com/blog/ai-tools-for-therapists/" target="_blank" rel="noopener">AI tools for therapists</a> in tandem with their own techniques to ensure that your mental health stays grounded in human expertise.&nbsp;&nbsp;</p>



  <div style="height:30px" aria-hidden="true" class="wp-block-spacer"></div>



  <p><em>Sources:</em></p>
  </div></div>



  <ol>
  <li>Berk H. Beware of Artificial Intelligence hallucinations or should we call confabulation?. <em>Acta Orthop Traumatol Turc</em>. 2024;58(1):1-3. doi:10.5152/j.aott.2024.130224 <a target="_blank" rel="noopener" href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11059964/">https://pmc.ncbi.nlm.nih.gov/articles/PMC11059964/</a></li>



  <li>Ji Z, Lee N, Frieske R, et al. Survey of hallucination in natural language generation. <em>ACM Comput. Surv.</em> 2023;55(12):1-38. doi:10.1145/3571730 <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3571730">https://dl.acm.org/doi/10.1145/3571730</a>&nbsp;</li>



  <li>OpenAI o3 and o4-mini System Card. OpenAI. Published April 16, 2025. Accessed September 30, 2025. <a target="_blank" rel="noopener" href="https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf">https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf</a>&nbsp;</li>



  <li>Morrin H, Nichols L, Levin M, et al. Delusions by design? How everyday AIs might be fuelling psychosis (and what can be done about it). <em>PsyArXiv Preprints</em>. 2025. doi/10.31234/osf.io/cmy7n_v5 <a target="_blank" rel="noopener" href="https://osf.io/preprints/psyarxiv/cmy7n_v5">https://osf.io/preprints/psyarxiv/cmy7n_v5</a></li>



  <li>Kosmyna N, Hauptmann E, Yuan YT, et al. Your brain on ChatGPT: Accumulation of cognitive debt when using an AI assistant for essay writing task. <em>ArXiv</em>. 2025. doi:10.48550/arXiv.2506.08872 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08872">https://arxiv.org/abs/2506.08872</a></li>
  </ol>
  <p>The post <a rel="nofollow" href="https://www.talkspace.com/blog/chatgpt-hallucinations/">The Dangers of ChatGPT Hallucinations</a> appeared first on <a rel="nofollow" href="https://www.talkspace.com/blog">Talkspace</a>.</p>
original_markdown_body: "**A note about AI:** _On the Talkspace blog we aim to provide
  trustworthy coverage of all the mental health topics people might be curious about,
  by delivering science-backed, clinician-reviewed information. Our articles on artificial
  intelligence (AI) and how this emerging technology may intersect with mental health
  and healthcare are designed to educate and add insights to this cultural conversation.
  We believe that therapy, at its core, is focused around the therapeutic connection
  between human therapists and our members. At Talkspace we only use ethical and responsible
  AI tools that are developed in partnership with our human clinicians. These tools
  aren’t designed to replace qualified therapists, but to enhance their ability to
  keep delivering high-quality care._ _To learn more, visit our_ [_AI-supported therapy
  page_](https://www.talkspace.com/ai-at-talkspace)_._\n\nArtificial intelligence
  (AI) tools, like ChatGPT, are becoming part of our daily lives. Many people use
  ChatGPT as an assistant to help brainstorm ideas, draft emails, or answer questions
  quickly. The fast and helpful responses can feel magical, making it tempting to
  blindly trust the information. What happens when ChatGPT gives you an answer that’s
  completely wrong?\n\nThe algorithms that power ChatGPT can sometimes produce false
  or misleading information that sounds convincing. This is called an AI hallucination.
  ChatGPT hallucinations can be dangerous if they influence your beliefs, emotions,
  or decisions. Continue reading to learn more about what causes ChatGPT hallucinations,
  why they’re harmful, and how to protect your mental well-being while using AI tools,
  like ChatGPT.&nbsp;\n\n## What Are AI “Hallucinations”?\n\nAI “hallucinations” happen
  when a ChatGPT bot produces information that sounds plausible, but it’s actually
  false, misleading, or unverifiable.&nbsp;\n\nExperts categorize hallucinations into
  two different types:\n\n- **Intrinsic hallucinations:** When the response generated
  by ChatGPT misrepresents or distorts information. For example, a chatbot may summarize
  a medical study in a way that changes its meaning or significance.&nbsp;\n- **Extrinsic
  hallucinations:** When ChatGPT generates information that isn’t backed up by real-world
  facts. For example, ChatGPT might invent a research study or statistic that doesn’t
  exist. An extrinsic hallucination doesn’t necessarily have to be wrong. However,
  the information isn’t verifiable from any known source or reference.&nbsp;\n\nIt
  can be difficult to spot a ChatGPT hallucination example because of how the response
  is delivered. ChatGPT hallucinations aren’t simple mistakes, like a typo. It’s an
  answer that seems polished, confident, and authoritative. We’re conditioned to think
  information is more persuasive when it’s written in this tone. Understanding what
  a ChatGPT hallucination is and knowing they can happen even when it sounds trustworthy
  is important to keep yourself safe when using this new technology.&nbsp;\n\n> “AI
  hallucinations can feel convincing because they exploit our brain’s tendency to
  trust confident, fluent, and human-like language—even when the content is entirely
  false.”\n> \n> <cite>– <a href=\"https://www.talkspace.com/mental-health/conditions/author/cynthia-v-catchings/\"
  target=\"_blank\" rel=\"noreferrer noopener\">Talkspace therapist Cynthia Catchings,
  PhD, LCSW-S</a></cite>\n\n## What Causes ChatGPT to Hallucinate?\n\nWhen you ask
  ChatGPT a question, the bot doesn’t think or fact-check the way a person does. Instead,
  it generates a response by predicting the most likely answer based on patterns it
  learned from being fed large amounts of text. While this process can often produce
  accurate answers, it’s not perfect and can lead to mistakes.&nbsp;\n\nThese mistakes
  can happen for a number of different reasons. ChatGPT is trained on both reliable
  and unreliable information. That means that errors in the information it was trained
  on can also show up in the answers. Another reason for ChatGPT hallucinations is
  that when it doesn’t know an answer, it may “guess,” resulting in answers that sound
  real, but aren’t. ChatGPT prioritizes a natural-sounding response, even when the
  information isn’t accurate.&nbsp;\n\n## How Often Does ChatGPT Hallucinate?\n\nChatGPT
  can be impressive, but it makes mistakes more than you may realize. Tests done by
  OpenAI (the creator of ChatGPT) [found that](https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf)
  the ChatGPT hallucination rate was between 33% and 79%, depending on the type of
  test and the model used. OpenAI says the newer models hallucinate less often, but
  no AI model is completely free of this problem.&nbsp;\n\n## When AI Hallucinations
  Become Dangerous\n\nIn some cases, you might be able to brush off a ChatGPT hallucination
  as a quirky mistake. However, when you begin to rely on ChatGPT for more serious
  information, hallucinations can have a bigger effect. False or misleading answers
  can affect decisions, health, and how you think.&nbsp;\n\n### Misinformation and
  poor decisions\n\nIf you assume ChatGPT is always reliable, you may use it to guide
  major decisions that affect your life.&nbsp;\n\nFor example, when using ChatGPT
  for health advice, acting on a hallucination could lead you to take actions that
  worsen your illness or delay you getting the treatment you need.&nbsp;&nbsp;\n\nAsking
  ChatGPT about how to invest your money or file your taxes could result in incorrect
  information that directly impacts your finances.&nbsp;\n\nUsing ChatGPT in legal
  scenarios can also have major implications. There have already been cases where
  AI hallucinations have shown up in legal cases by citing cases that don’t exist.
  If you’re relying on this information to defend your case in court, there could
  be serious consequences.&nbsp;\n\n### Mental health impacts\n\nChatGPT hallucinations
  can also affect your mental health. If you get an answer from a chatbot that’s unexpected
  or contradicts what you know, it can cause anxiety or confusion. You might find
  yourself questioning your own memory or judgment, wondering if you’re the one who’s
  misunderstanding some of the information provided.&nbsp;\n\n> “People may recognize
  that AI use is starting to negatively impact their thinking or emotional state when
  they feel anxious, dependent, or detached from real-life interactions, or when they
  start trusting AI responses over their own intuition or human relationships.”\n>
  \n> <cite>– <a href=\"https://www.talkspace.com/mental-health/conditions/author/cynthia-v-catchings/\"
  target=\"_blank\" rel=\"noreferrer noopener\">Talkspace therapist Cynthia Catchings,
  PhD, LCSW-S</a></cite>\n\nThere have also been emerging reports of [AI-induced psychosis](https://www.talkspace.com/blog/ai-psychosis/)
  where reliance on AI tools contributes to losing touch with reality. Since AI is
  still new, researchers don’t know the full extent to which this new technology might
  affect mental health.&nbsp;\n\n### Overreliance and loss of critical thinking\n\nWhen
  you start using ChatGPT and start getting confident, polished responses, it’s easy
  to rely on this information more and more. You may have even started by double-checking
  the system’s responses for accuracy. If you’ve found the information to be accurate
  in the past, you may stop fact-checking or questioning the responses in the future.
  Over time, this can weaken your own critical thinking and decision-making skills.&nbsp;\n\nA
  [2025 study](https://arxiv.org/abs/2506.08872) found that students who used ChatGPT
  to write an essay had lower brain activity and underperformed in their tasks compared
  to students who didn’t use AI tools. Although ChatGPT can be convenient, it may
  come at the cost of accuracy and your critical thinking skills.&nbsp;\n\n### Dependency
  on AI for reassurance\n\nIf you’ve found ChatGPT to be helpful in completing your
  daily tasks, you may also wonder about its ability to provide reassurance and emotional
  support. Some people are turning to [AI for mental health support](https://www.talkspace.com/blog/ai-mental-health/),
  even going as far as using [ChatGPT as a therapist](https://www.talkspace.com/blog/chatgpt-therapist/).&nbsp;\n\nDepending
  on ChatGPT to help validate your feelings or guide personal decisions may reduce
  your confidence in your own judgment and that of your loved ones whom you normally
  confide in. It’s important to remember that while ChatGPT can be a helpful sounding
  board, it’s not a replacement for real human connection.&nbsp;\n\n## Who Is Most
  at Risk?\n\nAnyone who uses ChatGPT may encounter a hallucination. If your prompt
  or the question you asked is low-stakes, a ChatGPT hallucination may present an
  inconvenience. However, in high-stakes situations that involve your health or safety,
  a hallucination can pose a big risk.&nbsp;\n\nPeople who spend significant time
  engaging with ChatGPT may have a higher risk of encountering a hallucination that
  causes harm. For example, people who turn to ChatGPT for companionship or emotional
  validation may be at risk for more harm from a hallucination.&nbsp;\n\nAnother group
  of people who may be more vulnerable to harm from ChatGPT hallucinations is those
  with preexisting mental health conditions, like anxiety or psychosis. Someone who
  already has a mental health condition may become confused or distressed by an incorrect
  or misleading AI hallucination.&nbsp;\n\n## How To Use ChatGPT Responsibly & Avoid
  Risks\n\nChatGPT can be a helpful tool when it’s used responsibly. It’s important
  to remember that no matter how advanced the technology is, it’s not perfect, and
  it can make mistakes. That doesn’t mean you shouldn’t use ChatGPT, but you should
  be aware of some ways to help reduce your risk of harm from ChatGPT hallucinations.&nbsp;\n\n###
  Verify information\n\nThe best way to stay safe when using ChatGPT is to double-check
  the information it gives you. Even when the answers sound confident, be aware that
  they could be incomplete, misleading, or outright wrong in some cases.&nbsp;\n\nCross-check
  the answer with credible sources before making any decisions. Look for consistency
  with trusted professional advice. For financial guidance, consult official government
  resources or a licensed financial advisor. If you have legal questions, your best
  source is a qualified attorney.&nbsp;\n\nTo help verify health information, you
  can check with trusted health sources, such as those from large hospitals or the
  National Institutes of Health. Even after checking with trusted sources, it’s important
  to talk to a healthcare provider if you have any concerns.&nbsp;\n\nIt’s important
  to understand that extrinsic AI hallucinations may not be verifiable. If you can’t
  find information to back up an answer from ChatGPT, it may be a hallucination.&nbsp;\n\n###
  Set boundaries on use\n\nSpending too much time using ChatGPT can put you at risk
  of harm from a ChatGPT hallucination. To help avoid this, limit prolonged sessions
  and try to avoid late-night sessions where fatigue can make it harder to think critically.&nbsp;\n\nIt’s
  also important to pay attention to how your interactions with ChatGPT affect your
  mood and how you think. If you notice that you’re feeling more anxiety, self-doubt,
  or frustration, it may be a sign that you need to take a step back from your ChatGPT
  use.&nbsp;\n\n### Treat AI as a supplement, not a substitute\n\nChatGPT is a great
  tool when it’s used for things like brainstorming or as a support to your work.
  However, the algorithms that power ChatGPT and similar AI models aren’t a replacement
  for real human expertise shaped over years of experience. If you’re planning on
  making important decisions, make sure to keep human oversight central to your decision-making
  process.&nbsp;\n\n## Leaning on Humans for Support\n\nChatGPT and other types of
  AI can be helpful tools, but they’re not a substitute for real human guidance. If
  you ever feel like your use of ChatGPT is leaving you anxious, confused, or unable
  to trust your own judgment, it may be a sign to reach out for professional guidance.
  A licensed therapist can give you coping strategies, emotional support, and a human
  perspective that ChatGPT simply can’t provide.&nbsp;\n\nWhen it comes to the use
  of [AI at Talkspace](https://www.talkspace.com/ai-at-talkspace), these tools are
  used to enhance human-first care, not replace it. With [online therapy](https://www.talkspace.com/online-therapy/),
  you can connect with a licensed therapist who can help you navigate the challenges
  of daily life and prioritize your well-being. With Talkspace, you can turn to a
  real person using the best [AI tools for therapists](https://www.talkspace.com/blog/ai-tools-for-therapists/)
  in tandem with their own techniques to ensure that your mental health stays grounded
  in human expertise.&nbsp;&nbsp;\n\n_Sources:_\n\n1. Berk H. Beware of Artificial
  Intelligence hallucinations or should we call confabulation?. _Acta Orthop Traumatol
  Turc_. 2024;58(1):1-3. doi:10.5152/j.aott.2024.130224 [https://pmc.ncbi.nlm.nih.gov/articles/PMC11059964/](https://pmc.ncbi.nlm.nih.gov/articles/PMC11059964/)\n2.
  Ji Z, Lee N, Frieske R, et al. Survey of hallucination in natural language generation.
  _ACM Comput. Surv._ 2023;55(12):1-38. doi:10.1145/3571730 [https://dl.acm.org/doi/10.1145/3571730](https://dl.acm.org/doi/10.1145/3571730)&nbsp;\n3.
  OpenAI o3 and o4-mini System Card. OpenAI. Published April 16, 2025. Accessed September
  30, 2025. [https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf](https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf)&nbsp;\n4.
  Morrin H, Nichols L, Levin M, et al. Delusions by design? How everyday AIs might
  be fuelling psychosis (and what can be done about it). _PsyArXiv Preprints_. 2025.
  doi/10.31234/osf.io/cmy7n\\_v5 [https://osf.io/preprints/psyarxiv/cmy7n\\_v5](https://osf.io/preprints/psyarxiv/cmy7n_v5)\n5.
  Kosmyna N, Hauptmann E, Yuan YT, et al. Your brain on ChatGPT: Accumulation of cognitive
  debt when using an AI assistant for essay writing task. _ArXiv_. 2025. doi:10.48550/arXiv.2506.08872
  [https://arxiv.org/abs/2506.08872](https://arxiv.org/abs/2506.08872)\n\nThe post
  [The Dangers of ChatGPT Hallucinations](https://www.talkspace.com/blog/chatgpt-hallucinations/)
  appeared first on [Talkspace](https://www.talkspace.com/blog)."
source: Talkspace Free Online Therapy for Seattle Youth
source_url: https://www.talkspace.com/blog/chatgpt-hallucinations/
summarized: true
title: The Dangers of ChatGPT Hallucinations
topics:
- Mental Health & Counseling
- Legal Aid
---

Artificial intelligence tools like ChatGPT have become integral in daily tasks, but they can produce misleading or false information, referred to as AI hallucinations. These hallucinations occur when the AI generates responses that sound credible but are incorrect or unverifiable. There are two types: intrinsic hallucinations, which misrepresent existing information, and extrinsic hallucinations, which create unverifiable facts. The polished and confident delivery of these responses can make it hard to discern their accuracy.

Research suggests that hallucinations can happen frequently, with rates ranging from 33% to 79%, depending on various factors. Relying on ChatGPT for critical information, especially about health or legal matters, can lead to poor decisions. Additionally, misinformation can affect mental health, causing anxiety and confusion, particularly for users with preexisting conditions.

To mitigate risks, users should verify AI-generated information with credible sources and maintain a healthy boundary with technology use. AI tools should enhance human expertise rather than replace it, emphasizing the importance of human therapists for emotional support and guidance.
