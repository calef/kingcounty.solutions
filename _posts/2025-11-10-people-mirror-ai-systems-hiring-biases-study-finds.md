---
title: People mirror AI systems’ hiring biases, study finds
date: '2025-11-10T15:46:33+00:00'
source: University of Washington
source_url: https://www.washington.edu/news/2025/11/10/people-mirror-ai-systems-hiring-biases-study-finds/
original_content: |-
  <div id="attachment_89411" class="wp-caption aligncenter" style="width: 1150px"><img loading="lazy" decoding="async" class="wp-image-89411 size-page" src="https://uw-s3-cdn.s3.us-west-2.amazonaws.com/wp-content/uploads/sites/6/2025/09/25111643/iStock-1352603244-1140x655.jpg" alt="A person's hands type on a laptop." width="1140" height="655" srcset="https://uw-s3-cdn.s3.us-west-2.amazonaws.com/wp-content/uploads/sites/6/2025/09/25111643/iStock-1352603244-1140x655.jpg 1140w, https://uw-s3-cdn.s3.us-west-2.amazonaws.com/wp-content/uploads/sites/6/2025/09/25111643/iStock-1352603244-300x172.jpg 300w, https://uw-s3-cdn.s3.us-west-2.amazonaws.com/wp-content/uploads/sites/6/2025/09/25111643/iStock-1352603244-1024x589.jpg 1024w, https://uw-s3-cdn.s3.us-west-2.amazonaws.com/wp-content/uploads/sites/6/2025/09/25111643/iStock-1352603244-768x441.jpg 768w, https://uw-s3-cdn.s3.us-west-2.amazonaws.com/wp-content/uploads/sites/6/2025/09/25111643/iStock-1352603244-1536x883.jpg 1536w, https://uw-s3-cdn.s3.us-west-2.amazonaws.com/wp-content/uploads/sites/6/2025/09/25111643/iStock-1352603244-2048x1177.jpg 2048w, https://uw-s3-cdn.s3.us-west-2.amazonaws.com/wp-content/uploads/sites/6/2025/09/25111643/iStock-1352603244-375x216.jpg 375w, https://uw-s3-cdn.s3.us-west-2.amazonaws.com/wp-content/uploads/sites/6/2025/09/25111643/iStock-1352603244-750x431.jpg 750w" sizes="auto, (max-width: 1140px) 100vw, 1140px" /></p>
  <p class="wp-caption-text">In a new University of Washington study, 528 people worked with simulated LLMs to pick candidates for 16 different jobs, from computer systems analyst to nurse practitioner to housekeeper. The researchers simulated different levels of racial biases in LLM recommendations for resumes from equally qualified white, Black, Hispanic and Asian men.<span class="wp-media-credit">Delmaine Donson/iStock</span></span></p>
  </div>
  <p><span style="font-weight: 400">An organization drafts a job listing with artificial intelligence. Droves of </span><a href="https://www.theatlantic.com/ideas/archive/2025/09/job-market-hell/684133/"><span style="font-weight: 400">applicants conjure resumes and cover letters</span></a><span style="font-weight: 400"> with chatbots. Another AI system sifts through those applications, passing recommendations to hiring managers. Perhaps AI avatars conduct screening interviews. This is increasingly the state of hiring, as people seek to streamline the stressful, tedious process with AI.</span></p>
  <p><span style="font-weight: 400">Yet research is finding that hiring bias — against people with disabilities, or certain races and genders — permeates large language models, or LLMs, such as ChatGPT and Gemini. We know less, though, about how biased LLM recommendations influence the people making hiring decisions. </span></p>
  <p><span style="font-weight: 400">In a new University of Washington study, 528 people worked with simulated LLMs to pick candidates for 16 different jobs, from computer systems analyst to nurse practitioner to housekeeper. The researchers simulated different levels of racial biases in LLM recommendations for resumes from equally qualified white, Black, Hispanic and Asian men. </span></p>
  <p><span style="font-weight: 400">When picking candidates without AI or with neutral AI, participants picked white and non-white applicants at equal rates. But when they worked with a moderately biased AI, if the AI preferred non-white candidates, participants did too. If it preferred white candidates, participants did too. In cases of severe bias, people made only slightly less biased decisions than the recommendations.</span></p>
  <p><span style="font-weight: 400">The team <a href="https://ojs.aaai.org/index.php/AIES/article/view/36749">presented its findings</a> Oct. 22 at the AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society in Madrid. </span></p>
  <p><span style="font-weight: 400">“In one survey, 80% of organizations using AI hiring tools said they don’t reject applicants without human review,” said lead author </span><a href="https://kyrawilson.github.io/me/"><span style="font-weight: 400">Kyra Wilson</span></a><span style="font-weight: 400">, a UW doctoral student in the Information School. “So this human-AI interaction is the dominant model right now. Our goal was to take a critical look at this model and see how human reviewers’ decisions are being affected. Our findings were stark: Unless bias is obvious, people were perfectly willing to accept the AI’s biases.”</span></p>
  <div id="attachment_89840" class="wp-caption aligncenter" style="width: 935px"><img loading="lazy" decoding="async" class="size-full wp-image-89840" src="https://uw-s3-cdn.s3.us-west-2.amazonaws.com/wp-content/uploads/sites/6/2025/11/10075706/Screenshot-2025-09-08-at-2.00.49-PM-1.png" alt="" width="925" height="745" srcset="https://uw-s3-cdn.s3.us-west-2.amazonaws.com/wp-content/uploads/sites/6/2025/11/10075706/Screenshot-2025-09-08-at-2.00.49-PM-1.png 925w, https://uw-s3-cdn.s3.us-west-2.amazonaws.com/wp-content/uploads/sites/6/2025/11/10075706/Screenshot-2025-09-08-at-2.00.49-PM-1-300x242.png 300w, https://uw-s3-cdn.s3.us-west-2.amazonaws.com/wp-content/uploads/sites/6/2025/11/10075706/Screenshot-2025-09-08-at-2.00.49-PM-1-768x619.png 768w, https://uw-s3-cdn.s3.us-west-2.amazonaws.com/wp-content/uploads/sites/6/2025/11/10075706/Screenshot-2025-09-08-at-2.00.49-PM-1-375x302.png 375w, https://uw-s3-cdn.s3.us-west-2.amazonaws.com/wp-content/uploads/sites/6/2025/11/10075706/Screenshot-2025-09-08-at-2.00.49-PM-1-750x604.png 750w" sizes="auto, (max-width: 925px) 100vw, 925px" /></p>
  <p class="wp-caption-text">Participants were given a job description and the names and resumes of five candidates: two white men; two men who were either Asian, Black or Hispanic; and one candidate whose resume lacked qualifications for the job, to obscure the purpose of the study. An example from the study is shown here.<span class="wp-media-credit">Wilson et al./AIES ‘25</span></span></p>
  </div>
  <p><span style="font-weight: 400">The team recruited 528 online participants from the U.S. through surveying platform </span><a href="https://www.prolific.com/"><span style="font-weight: 400">Prolific</span></a><span style="font-weight: 400">, who were then asked to screen job applicants. They were given a job description and the names and resumes of five candidates: two white men and two men who were either Asian, Black or Hispanic. These four were equally qualified. To obscure the purpose of the study, the final candidate was of a race not being compared and lacked qualifications for the job. Candidates’ names implied their races — for example, Gary O’Brien for a white candidate. Affinity groups, such as Asian Student Union Treasurer, also signaled race.</span></p>
  <p><span style="font-weight: 400">In four trials, the participants picked three of the five candidates to interview. In the first trial, the AI provided no recommendation. In the next trials, the AI recommendations were neutral (one candidate of each race), severely biased (candidates from only one race), or moderately biased, meaning candidates were recommended at rates similar to rates of bias in real AI models. The team derived rates of moderate bias </span><a href="https://www.washington.edu/news/2024/10/31/ai-bias-resume-screening-race-gender/"><span style="font-weight: 400">using the same methods as in their 2024 study that looked at bias in three common AI systems</span></a><span style="font-weight: 400">. </span></p>
  <p><span style="font-weight: 400">Rather than having participants interact directly with the AI system, the team simulated the AI interactions so they could hew to rates of bias from their large-scale study. Researchers also used AI generated resumes, rather than real resumes, which they validated. This allowed greater control, and AI-written resumes are increasingly common in hiring.</span></p>
  <p><span style="font-weight: 400">“Getting access to real-world hiring data is almost impossible, given the sensitivity and privacy concerns,” said senior author </span><a href="https://faculty.washington.edu/aylin/"><span style="font-weight: 400">Aylin Caliskan</span></a><span style="font-weight: 400">, a UW associate professor in the Information School. “But this lab experiment allowed us to carefully control the study and learn new things about bias in human-AI interaction.”</span></p>
  <div class="info-box info-box-large">
  <p><span style="font-weight: 400">Related:</span></p>
  <ul>
  <li style="font-weight: 400"><a href="https://www.washington.edu/news/2024/06/21/chatgpt-ai-bias-ableism-disability-resume-cv/"><span style="font-weight: 400">ChatGPT is biased against resumes with credentials that imply a disability — but it can improve</span></a></li>
  <li style="font-weight: 400"><a href="https://www.washington.edu/news/2024/10/31/ai-bias-resume-screening-race-gender/"><span style="font-weight: 400">AI tools show biases in ranking job applicants’ names according to perceived race and gender</span></a></li>
  </ul>
  </div>
  <p><span style="font-weight: 400">Without suggestions, participants’ choices exhibited little bias. But when provided with recommendations, participants mirrored the AI. In the case of severe bias, choices followed the AI picks around 90% of the time, rather than nearly all the time, indicating that even if people are able to recognize AI bias, that awareness isn’t strong enough to negate it.</span></p>
  <p><span style="font-weight: 400">“There is a bright side here,” Wilson said. “If we can tune these models appropriately, then it&#8217;s more likely that people are going to make unbiased decisions themselves. Our work highlights a few possible paths forward.”</span></p>
  <p><span style="font-weight: 400">In the study, bias dropped 13% when participants began with an </span><a href="https://en.wikipedia.org/wiki/Implicit-association_test"><span style="font-weight: 400">implicit association test</span></a><span style="font-weight: 400">, intended to detect subconscious bias. So companies including such tests in hiring trainings may mitigate biases. Educating people about AI can also improve awareness of its limitations.</span></p>
  <p><span style="font-weight: 400">“People have agency, and that has huge impact and consequences, and we shouldn&#8217;t lose our critical thinking abilities when interacting with AI,” Caliskan said. “But I don’t want to place all the responsibility on people using AI. The scientists building these systems know the risks and need to work to reduce systems’ biases. And we need policy, obviously, so that models can be aligned with societal and organizational values.”</span></p>
  <p><a href="https://www.linkedin.com/in/anna-gueorguieva-a60183196/"><i><span style="font-weight: 400">Anna-Maria Gueorguieva</span></i></a><i><span style="font-weight: 400">, a UW doctoral student in the Information School, and </span></i><a href="https://prism.eng.ufl.edu/about-us/students/"><i><span style="font-weight: 400">Mattea Sim</span></i></a><i><span style="font-weight: 400">, a postdoctoral scholar at Indiana University, are also co-authors on this paper. This research was funded by The U.S. National Institute of Standards and Technology.</span></i></p>
  <p><i><span style="font-weight: 400">For more information, contact Wilson at </span></i><a href="mailto:kywi@uw.edu"><i><span style="font-weight: 400">kywi@uw.edu</span></i></a><i><span style="font-weight: 400"> and Caliskan at </span></i><a href="mailto:aylin@uw.edu"><i><span style="font-weight: 400">aylin@uw.edu</span></i></a><i><span style="font-weight: 400">. </span></i></p>
  <p><a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fwww.washington.edu%2Fcms%2Fnews%2F2025%2F11%2F10%2Fpeople-mirror-ai-systems-hiring-biases-study-finds%2F&amp;linkname=People%20mirror%20AI%20systems%E2%80%99%20hiring%20biases%2C%20study%20finds" title="Facebook" rel="nofollow noopener" target="_blank"></a><a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fwww.washington.edu%2Fcms%2Fnews%2F2025%2F11%2F10%2Fpeople-mirror-ai-systems-hiring-biases-study-finds%2F&amp;linkname=People%20mirror%20AI%20systems%E2%80%99%20hiring%20biases%2C%20study%20finds" title="Twitter" rel="nofollow noopener" target="_blank"></a><a class="a2a_button_reddit a2a_counter" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fwww.washington.edu%2Fcms%2Fnews%2F2025%2F11%2F10%2Fpeople-mirror-ai-systems-hiring-biases-study-finds%2F&amp;linkname=People%20mirror%20AI%20systems%E2%80%99%20hiring%20biases%2C%20study%20finds" title="Reddit" rel="nofollow noopener" target="_blank"></a><a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fwww.washington.edu%2Fcms%2Fnews%2F2025%2F11%2F10%2Fpeople-mirror-ai-systems-hiring-biases-study-finds%2F&amp;linkname=People%20mirror%20AI%20systems%E2%80%99%20hiring%20biases%2C%20study%20finds" title="Email" rel="nofollow noopener" target="_blank"></a><a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fwww.washington.edu%2Fcms%2Fnews%2F2025%2F11%2F10%2Fpeople-mirror-ai-systems-hiring-biases-study-finds%2F&amp;linkname=People%20mirror%20AI%20systems%E2%80%99%20hiring%20biases%2C%20study%20finds" title="Print" rel="nofollow noopener" target="_blank"></a><a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fwww.washington.edu%2Fcms%2Fnews%2F2025%2F11%2F10%2Fpeople-mirror-ai-systems-hiring-biases-study-finds%2F&#038;title=People%20mirror%20AI%20systems%E2%80%99%20hiring%20biases%2C%20study%20finds" data-a2a-url="https://www.washington.edu/news/2025/11/10/people-mirror-ai-systems-hiring-biases-study-finds/" data-a2a-title="People mirror AI systems’ hiring biases, study finds"></a></p>
original_markdown_body: |-
  ![A person's hands type on a laptop.](https://uw-s3-cdn.s3.us-west-2.amazonaws.com/wp-content/uploads/sites/6/2025/09/25111643/iStock-1352603244-1140x655.jpg)

  In a new University of Washington study, 528 people worked with simulated LLMs to pick candidates for 16 different jobs, from computer systems analyst to nurse practitioner to housekeeper. The researchers simulated different levels of racial biases in LLM recommendations for resumes from equally qualified white, Black, Hispanic and Asian men.Delmaine Donson/iStock

  An organization drafts a job listing with artificial intelligence. Droves of [applicants conjure resumes and cover letters](https://www.theatlantic.com/ideas/archive/2025/09/job-market-hell/684133/) with chatbots. Another AI system sifts through those applications, passing recommendations to hiring managers. Perhaps AI avatars conduct screening interviews. This is increasingly the state of hiring, as people seek to streamline the stressful, tedious process with AI.

  Yet research is finding that hiring bias — against people with disabilities, or certain races and genders — permeates large language models, or LLMs, such as ChatGPT and Gemini. We know less, though, about how biased LLM recommendations influence the people making hiring decisions.&nbsp;

  In a new University of Washington study, 528 people worked with simulated LLMs to pick candidates for 16 different jobs, from computer systems analyst to nurse practitioner to housekeeper. The researchers simulated different levels of racial biases in LLM recommendations for resumes from equally qualified white, Black, Hispanic and Asian men.&nbsp;

  When picking candidates without AI or with neutral AI, participants picked white and non-white applicants at equal rates. But when they worked with a moderately biased AI, if the AI preferred non-white candidates, participants did too. If it preferred white candidates, participants did too. In cases of severe bias, people made only slightly less biased decisions than the recommendations.

  The team [presented its findings](https://ojs.aaai.org/index.php/AIES/article/view/36749) Oct. 22 at the AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society in Madrid.&nbsp;

  “In one survey, 80% of organizations using AI hiring tools said they don’t reject applicants without human review,” said lead author [Kyra Wilson](https://kyrawilson.github.io/me/), a UW doctoral student in the Information School. “So this human-AI interaction is the dominant model right now. Our goal was to take a critical look at this model and see how human reviewers’ decisions are being affected. Our findings were stark: Unless bias is obvious, people were perfectly willing to accept the AI’s biases.”

   ![](https://uw-s3-cdn.s3.us-west-2.amazonaws.com/wp-content/uploads/sites/6/2025/11/10075706/Screenshot-2025-09-08-at-2.00.49-PM-1.png)

  Participants were given a job description and the names and resumes of five candidates: two white men; two men who were either Asian, Black or Hispanic; and one candidate whose resume lacked qualifications for the job, to obscure the purpose of the study. An example from the study is shown here.Wilson et al./AIES ‘25

  The team recruited 528 online participants from the U.S. through surveying platform [Prolific](https://www.prolific.com/), who were then asked to screen job applicants. They were given a job description and the names and resumes of five candidates: two white men and two men who were either Asian, Black or Hispanic. These four were equally qualified. To obscure the purpose of the study, the final candidate was of a race not being compared and lacked qualifications for the job. Candidates’ names implied their races — for example, Gary O’Brien for a white candidate. Affinity groups, such as Asian Student Union Treasurer, also signaled race.

  In four trials, the participants picked three of the five candidates to interview. In the first trial, the AI provided no recommendation. In the next trials, the AI recommendations were neutral (one candidate of each race), severely biased (candidates from only one race), or moderately biased, meaning candidates were recommended at rates similar to rates of bias in real AI models. The team derived rates of moderate bias [using the same methods as in their 2024 study that looked at bias in three common AI systems](https://www.washington.edu/news/2024/10/31/ai-bias-resume-screening-race-gender/).&nbsp;

  Rather than having participants interact directly with the AI system, the team simulated the AI interactions so they could hew to rates of bias from their large-scale study. Researchers also used AI generated resumes, rather than real resumes, which they validated. This allowed greater control, and AI-written resumes are increasingly common in hiring.

  “Getting access to real-world hiring data is almost impossible, given the sensitivity and privacy concerns,” said senior author [Aylin Caliskan](https://faculty.washington.edu/aylin/), a UW associate professor in the Information School. “But this lab experiment allowed us to carefully control the study and learn new things about bias in human-AI interaction.”

  Related:

  - [ChatGPT is biased against resumes with credentials that imply a disability — but it can improve](https://www.washington.edu/news/2024/06/21/chatgpt-ai-bias-ableism-disability-resume-cv/)
  - [AI tools show biases in ranking job applicants’ names according to perceived race and gender](https://www.washington.edu/news/2024/10/31/ai-bias-resume-screening-race-gender/)

  Without suggestions, participants’ choices exhibited little bias. But when provided with recommendations, participants mirrored the AI. In the case of severe bias, choices followed the AI picks around 90% of the time, rather than nearly all the time, indicating that even if people are able to recognize AI bias, that awareness isn’t strong enough to negate it.

  “There is a bright side here,” Wilson said. “If we can tune these models appropriately, then it’s more likely that people are going to make unbiased decisions themselves. Our work highlights a few possible paths forward.”

  In the study, bias dropped 13% when participants began with an [implicit association test](https://en.wikipedia.org/wiki/Implicit-association_test), intended to detect subconscious bias. So companies including such tests in hiring trainings may mitigate biases. Educating people about AI can also improve awareness of its limitations.

  “People have agency, and that has huge impact and consequences, and we shouldn’t lose our critical thinking abilities when interacting with AI,” Caliskan said. “But I don’t want to place all the responsibility on people using AI. The scientists building these systems know the risks and need to work to reduce systems’ biases. And we need policy, obviously, so that models can be aligned with societal and organizational values.”

  [_Anna-Maria Gueorguieva_](https://www.linkedin.com/in/anna-gueorguieva-a60183196/)_, a UW doctoral student in the Information School, and_ [_Mattea Sim_](https://prism.eng.ufl.edu/about-us/students/)_, a postdoctoral scholar at Indiana University, are also co-authors on this paper. This research was funded by The U.S. National Institute of Standards and Technology._

  _For more information, contact Wilson at_ [_kywi@uw.edu_](mailto:kywi@uw.edu) _and Caliskan at_ [_aylin@uw.edu_](mailto:aylin@uw.edu)_._
summarized: true
topics: []
published: false
---

A study by the University of Washington found that human hiring decisions closely mirror the biases present in artificial intelligence (AI) systems. Involving 528 participants who evaluated candidates for various jobs, the research simulated different levels of racial bias in AI recommendations. When participants made hiring decisions without AI assistance or with neutral AI, their choices showed little bias. However, when using a moderately biased AI, participants adopted the AI's preferences, favoring non-white candidates if the AI did, and vice versa. In cases of severe bias, participants' decisions aligned nearly 90% with the AI's recommendations.

The study highlights a concerning trend where human judgment is influenced by AI biases, especially if those biases are not overtly recognized. Participants were provided with resumes suggesting racial backgrounds, and those exposed to AI recommendations exhibited significantly biased decision-making. The research suggests that incorporating implicit association tests in training could help reduce biases, indicating that both awareness and the design of AI systems are critical for fair hiring practices. The findings were presented at the AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society in Madrid.